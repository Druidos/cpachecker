{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License\n",
    "---\n",
    "This file is part of CPAchecker,<br>\n",
    "a tool for configurable software verification:<br>\n",
    "https://cpachecker.sosy-lab.org<br>\n",
    "<br>\n",
    "SPDX-FileCopyrightText: 2020 Dirk Beyer <https://www.sosy-lab.org><br>\n",
    "<br>\n",
    "SPDX-License-Identifier: Apache-2.0<br>\n",
    "<br>\n",
    "\n",
    "Strategy Selection\n",
    "---\n",
    "*description*\n",
    "\n",
    "---\n",
    "The format of the input should look like this:\n",
    "```python\n",
    "result_sets = [(anaylsis1results, time1),..., (analysisNresults, timeN)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Enter your result sets and the time limit in the next cell as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sets = [('slicing-results/slicing-predicate.2020-11-06_16-54-13.results.reference.ReachSafety-Arrays.xml.bz2', 500), \n",
    "               ('slicing-results/slicing-predicate.2020-11-06_16-54-13.results.reference.ReachSafety-Arrays.xml.bz2', 400)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell provides the util methods to convert the result sets to pandas dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchexec import tablegenerator\n",
    "import benchexec.result\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "\n",
    "def get_data(data_files: Union[List[str], str]) -> List[pd.DataFrame]:\n",
    "    if not isinstance(data_files, str):\n",
    "        return [d for f in data_files for d in get_data(f)]\n",
    "    raw_data = _load_data(data_files)\n",
    "    return [pd.DataFrame(data=_get_values(raw_data), columns=_get_column_titles(raw_data))]\n",
    "\n",
    "\n",
    "def _load_data(data_file: str) -> tablegenerator.RunSetResult:\n",
    "    parser = tablegenerator.create_argument_parser()\n",
    "    options = parser.parse_args([data_file])\n",
    "    return tablegenerator.load_result(data_file, options)\n",
    "\n",
    "\n",
    "def _get_column_titles(result_set: tablegenerator.RunSetResult):\n",
    "    return ['task_id', 'category'] + [col.title for col in result_set.columns]\n",
    "\n",
    "\n",
    "def _get_values(result_set):\n",
    "    return [[r.task_id[0], r.category] + cast_values(r) for r in result_set.results]\n",
    "\n",
    "\n",
    "def cast_values(r: tablegenerator.RunResult):\n",
    "    return [to_value(value, column) for value, column in zip(r.values, r.columns)]\n",
    "\n",
    "\n",
    "def to_value(value, column):\n",
    "    if not column.is_numeric() or value is None:\n",
    "        return value\n",
    "    return float(value[:-len(column.unit)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are used to calculate the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestVector():\n",
    "    cat = ['task_id', 'category', 'status', 'cputime', 'walltime', 'memory', 'host']\n",
    "    test_vector1 = pd.DataFrame([['task1', 'correct', 'true', 200, 200, 0, 'a'], ['task2', 'error', 'ERROR', 300, 200, 0, 'a']], columns=cat)\n",
    "    test_vector2 = pd.DataFrame([['task1', 'error', 'ERROR', 200, 200, 0, 'a'], ['task2', 'correct', 'true', 100, 200, 0, 'a']], columns=cat)\n",
    "    return [(test_vector1, 150), (test_vector2, 400)]\n",
    "\n",
    "def merge_columns(category, status):\n",
    "    merge_dict = {\n",
    "        (\"correct\", \"true\"): \"TP\",\n",
    "        (\"incorrect\", \"true\"): \"FP\",\n",
    "        (\"correct\", \"false\"): \"TN\",\n",
    "        (\"incorrect\", \"true\"): \"FN\"\n",
    "    }\n",
    "    if (category, status) in merge_dict:\n",
    "        return merge_dict[(category, status)]\n",
    "    return \"ERR/UNKNOWN/TIMEOUT\"\n",
    "\n",
    "def prepare_sets(input_sets):\n",
    "    frames = []\n",
    "    for (result, time) in input_sets:\n",
    "        df = get_data(result)[0]\n",
    "    #Header to use test vector:\n",
    "    #for (result, time) in getTestVector():\n",
    "    #    df = result\n",
    "        df = df.drop(columns=['host'])\n",
    "        df['status'] = np.where(df['cputime'] > time, 'TIMEOUT', df['status'])\n",
    "        df['cputime'] = np.where(df['cputime'] > time, time, df['cputime'])\n",
    "        frames.append(df)\n",
    "    concat = pd.concat(frames)\n",
    "    concat['result'] = concat.apply(lambda row: merge_columns(row['category'], row['status']), axis=1)\n",
    "    return concat\n",
    "\n",
    "def agg_seq_result(result):\n",
    "    for elem in result:\n",
    "        if elem != \"ERR/UNKNOWN/TIMEOUT\":\n",
    "            return elem\n",
    "    return \"ERR/UNKNOWN/TIMEOUT\"\n",
    "\n",
    "#fast wrong cputime\n",
    "def sequential_deprecated():\n",
    "    results = prepare_sets()\n",
    "    results = results[['task_id', 'result', 'cputime']]\n",
    "    # group by id and return first result not equal to \"UNKNOWN\", sum cputime\n",
    "    results = results.groupby(results['task_id']).agg({'cputime': 'sum', 'result': lambda x: agg_seq_result(x)})\n",
    "    return results\n",
    "\n",
    "def sequential(input_sets):\n",
    "    prepared = prepare_sets(input_sets)\n",
    "    prepared = prepared[['task_id', 'result', 'cputime']]\n",
    "    distinct_ids = prepared.task_id.unique()\n",
    "    results = pd.DataFrame(columns = prepared.columns)\n",
    "    # loop through every distinct key\n",
    "    for key in distinct_ids:\n",
    "        selection = prepared[prepared['task_id'] == key]\n",
    "        col_result_distinct = selection.result.unique()\n",
    "        # if all results are unknown the tasks takes max(cputime)\n",
    "        if (len(col_result_distinct) == 1):\n",
    "            if (col_result_distinct[0] == \"ERR/UNKNOWN/TIMEOUT\"):\n",
    "                sum_time = selection['cputime'].sum()\n",
    "                new_row = pd.DataFrame([[key, \"ERR/UNKNOWN/TIMEOUT\", sum_time]], columns=prepared.columns)\n",
    "                results = results.append(new_row)\n",
    "                continue\n",
    "        # otherwise return result of first analysis that finishes min(cputime | result != \"UNKNOWN\")\n",
    "        col_result = \"\"\n",
    "        sum_time = 0\n",
    "        for index, row in selection.iterrows():\n",
    "            if (row[\"result\"] != \"ERR/UNKNOWN/TIMEOUT\"):\n",
    "                col_result = row[\"result\"]\n",
    "                sum_time = sum_time + row[\"cputime\"]\n",
    "                break\n",
    "            sum_time = sum_time + row[\"cputime\"]\n",
    "        new_row = pd.DataFrame([[key, col_result, sum_time]], columns=prepared.columns)\n",
    "        results = results.append(new_row)\n",
    "    return results\n",
    "\n",
    "def parallel(input_sets):\n",
    "    prepared = prepare_sets(input_sets)\n",
    "    prepared = prepared[['task_id', 'result', 'cputime']]\n",
    "    distinct_ids = prepared.task_id.unique()\n",
    "    results = pd.DataFrame(columns = prepared.columns)\n",
    "    # loop through every distinct key\n",
    "    for key in distinct_ids:\n",
    "        selection = prepared[prepared['task_id'] == key]\n",
    "        col_result_distinct = selection.result.unique()\n",
    "        # if all results are unknown the tasks takes max(cputime)\n",
    "        if (len(col_result_distinct) == 1):\n",
    "            if (col_result_distinct[0] == \"ERR/UNKNOWN/TIMEOUT\"):\n",
    "                max_time = selection['cputime'].max()\n",
    "                new_row = pd.DataFrame([[key, \"ERR/UNKNOWN/TIMEOUT\", max_time]], columns=prepared.columns)\n",
    "                results = results.append(new_row)\n",
    "                continue\n",
    "        # otherwise return result of first analysis that finishes min(cputime | result != \"UNKNOWN\")\n",
    "        selection = selection[selection['result'] != \"ERR/UNKNOWN/TIMEOUT\"]\n",
    "        selection = selection[selection['cputime'] == selection['cputime'].min()].head(1)\n",
    "        results = results.append(selection)\n",
    "    return results\n",
    "\n",
    "def score(df):\n",
    "    counts = df.result.value_counts()\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can choose your prefered strategy.\n",
    "Initialize\n",
    "```python\n",
    "# simulates the sequential execution of all analysis in result_sets\n",
    "# | analysis 0 | analysis 1 | ... | analysis n |\n",
    "strategy = sequential()\n",
    "```\n",
    "or\n",
    "```python\n",
    "# simulates the parallel execution of all analysis in result_sets\n",
    "# |      analysis 0     |\n",
    "# |        analysis 1        |\n",
    "# |       analysis 2       |\n",
    "# ...\n",
    "strategy = parallel()\n",
    "```\n",
    "to use the sequential or parallel strategy.\n",
    "Both analysis simulate an execution where the calculation terminates as soon as a result different from \"UNKNOWN\", \"ERROR\" or \"TIMEOUT\" occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = sequential(result_sets)\n",
    "#strategy = parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can rate your chosen timelimits. The output shows how often a result (e.g. \"UNKNOWN, \"TP\", \"FN\"...) occured. The score indicates the resulting SV-COMP rating for these results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERR/UNKNOWN/TIMEOUT    431\n",
       "TP                       5\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
